{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9wY4pib1YbyQjX/oQ+V37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinZejda/Summer2023_UCI_ML_Research/blob/main/4_Tutorial_convolutional_autoencoder_on_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder\n",
        "*   An unsupervised deep learning algorithm that learns encoded representations of the input data, then reconstructs the same input as output\n",
        "*   It consists of 2 networks: an encoder and a decoder\n",
        "*   **Encoder**: compresses a high-dimensional input into a low-dimensional latent code (aka encoded space) to extract the most relevant information from it\n",
        "*   **Decoder**: decompresses the encoded data, recreates the original input\n",
        "\n",
        "\n",
        "Goal of autoencoder architectures:\n",
        "*   to maximize the information when\n",
        " encoding, and minimizing the reconstruction error\n",
        "\n",
        "\n",
        "Reconstruction error:\n",
        "*   aka reconstruction loss\n",
        "*   it is the mean-squared error (MSE) between the reconstructed input and the original input when the input is real-valued (continuous). Otherwise, for categorical data we use the cross-entropy (CE) loss function instead.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f96sa93bbWaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation in PyTorch\n",
        "1. import libraries and MNIST dataset\n",
        "2. define convolutional autoencoder\n",
        "3. initialize loss function and optimizer\n",
        "4. train model and evaluate model\n",
        "5. generate new samples from the latent code\n",
        "6.   visualize the latent space with t-SNE\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvUxGlQGc8KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "xFLyO_TgdbDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SET UP MNIST DATA\n",
        "    # train data = training + validation (80%)\n",
        "    # test data = testing only, after training is done (20%)\n",
        "\n",
        "data_dir = 'dataset'\n",
        "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)   # download datasets\n",
        "test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
        "\n",
        "train_transform = transforms.Compose([transforms.ToTensor()])    # define transforms\n",
        "test_transform = transforms.Compose([transforms.ToTensor])       # we don't normalize bc these are colored images\n",
        "\n",
        "train_dataset.transform = train_transform                        # apply transforms to datasets\n",
        "test_dataset.transform = test_transform\n",
        "\n",
        "m = len(train_dataset)\n",
        "train_data, val_data = random_split(train_dataset, [int(m - m*0.2), int(m*0.2)])   # create 80-20 split of training data into: training + validation data\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "ALls-0YCd6xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE CONVOLUATIONAL AUTOENCODER\n",
        "    # composed of 2 classes: one for encoder, another for decoder\n",
        "    # encoder has 3 conv layers and 2 fc layers, with batch norm layers added as regularizers\n",
        "    # decoder has same architecture, but in inverse order\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoded_space_dim, fc2_input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    # convolutional section\n",
        "    self.encoder_cnn = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    # flatten layer\n",
        "    self.flatten = nn.Flatten(start_dim=1)\n",
        "\n",
        "    # linear section\n",
        "    self.encoder_lin = nn.Sequential(\n",
        "        nn.Linear(3 * 3 * 32, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(128, encoded_space_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder_cnn(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.encoder_lin(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):       # inverse architecture of Encoder\n",
        "\n",
        "  def __init__(self, encoded_space_dim, fc2_input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder_lin = nn.Sequential(\n",
        "        nn.Linear(encoded_space_dim, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(128, 3 * 3  * 32),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
        "\n",
        "    self.decoder_conv = nn.Sequential(\n",
        "        nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.decoder_lin(x)\n",
        "    x = self.unflatten(x)\n",
        "    x = self.decoder_conv(x)\n",
        "    x = torch.sigmoid(x)           # squishes values to between (0, 1), sigmoid(x) = 1 / (1 + e^-x), used for binary classification\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Xfofu8jRgIC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS FUNCTION AND OPTIMIZER\n",
        "loss_fn = torch.nn.MSELoss()     # loss function\n",
        "lr = 1e-3                        # learning rate\n",
        "\n",
        "torch.manual_seed(0)             # set seed for reproducible randomness\n",
        "\n",
        "d = 4\n",
        "encoder = Encoder(encoded_space_dim=d, fc2_input_dim=128)   # initialize the 2 networks\n",
        "decoder = Decoder(encoded_space_dim=d, fc2_input_dim=128)\n",
        "\n",
        "params_to_optimize = [\n",
        "    {'params' : encoder.parameters()},\n",
        "    {'params' : decoder.parameters()}\n",
        "]\n",
        "\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)  # adam optimizer\n",
        "\n",
        "# check if GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "# move both encoder and decoder to selected device\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "id": "AW69Iv-Ajmfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO TRAIN THE MODEL\n",
        "def train_epoch(encoder, decoder, dvice, dataloader, loss_fn, optimizer):\n",
        "\n",
        "  # set rain mode fro both the encoder and decoder\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  train_loss = list()\n",
        "\n",
        "  # iterate the dataloader\n",
        "  # we don't need labels here, this is unsupervised learning\n",
        "  # dataloader gives us tuple of (data, labels), we ignore labels with _\n",
        "  for image_batch, _ in dataloader:\n",
        "    image_batch = image_batch.to(device)         # move tensor to proper device\n",
        "\n",
        "    encoded_data = encoder(image_batch)          # encode data\n",
        "    decoded_data = decoder(encoded_data)         # decode data\n",
        "\n",
        "    loss = loss_fn(decoded_data, image_batch)    # this is really cool-- we compute loss by checking difference between decoded data and original data (to check how good the compression + decompression was)\n",
        "\n",
        "    optimizer.zero_grad()                        # backprop\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('\\t partial train loss (single batch): %f' %(loss.data))\n",
        "    train_loss.append(loss.detach.cpu().numpy())\n",
        "\n",
        "  return np.mean(train_loss)"
      ],
      "metadata": {
        "id": "gVb7uJJvjo4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO EVALUATE THE MODEL\n",
        "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
        "\n",
        "  encoder.eval()                  # set evaluation mode\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():           # don't change weights, aka don't track gradients\n",
        "\n",
        "    conc_out = list()             # lists to store outputs for each batch\n",
        "    conc_label = list()\n",
        "\n",
        "    for image_batch, _ in dataloader:\n",
        "\n",
        "      image_batch = image_batch.to(device)       # move tensor to proper device\n",
        "\n",
        "      encoded_data = encoder(image_batch)        # encode + decode data\n",
        "      decoded_data = decoder(encoded_data)\n",
        "\n",
        "      conc_out.append(decoded_data.cpu())        # append network output and original image to lists\n",
        "      conc_label.append(image_batch.cpu())\n",
        "\n",
        "    # create single tensor with all values in the lists\n",
        "    conc_out = torch.cat(conc_out)\n",
        "    conc_label = torch.cat(conc_label)\n",
        "\n",
        "    # evaluate global loss\n",
        "    val_loss = loss_fn(conc_out, conc_label)\n",
        "\n",
        "  return val_loss.data\n"
      ],
      "metadata": {
        "id": "d8LNH-oEm9Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO PLOT IMAGES\n",
        "# to see if the autoencoder is learning from the input images\n",
        "def plot_ae_outputs(encoder, decoder, n=10):\n",
        "\n",
        "  plt.figure(figsize=(16, 4.5))\n",
        "  targets = test_dataset.targets.numpy()\n",
        "  t_idx = {i : np.where(targets == i)[0][0] for i in range(n)}\n",
        "\n",
        "  for i in range(n):\n",
        "\n",
        "    ax = plt.subplot(2, n, i+1)\n",
        "    img = test_dataset[t_idx[i]][0].unsqueeze(0).to(device)        # extract ith image from test datset, then increase by 1 dimension on 0th axis, necessary to pass img to autoencoder\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      rec_img = decoder(encoder(img))                              # obtain reconstructed image\n",
        "\n",
        "    plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')      # plots the original image\n",
        "        # squeeze() to remove the dimension added before\n",
        "        # numpy() to transform the tensor into ndarray, so we can plt.imshow() it\n",
        "        # cpu() returns a copy of the tensor object into CPU memory\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    if i == n//2:\n",
        "      ax.set_title('Original images')\n",
        "\n",
        "    ax = plt.subplot(2, n, i+1+n)\n",
        "    plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  # plots the reconstructed image\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    if i == n//2:\n",
        "      ax.set_title('Reconstructed images')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KMxIap8NollY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERFORM TRAINING + EVALUATION\n",
        "num_epochs = 30\n",
        "diz_loss = {'train_loss':[], 'val_loss':[]}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_loss = train_epoch(encoder, decoder, device, train_loader, loss_fn, optim)\n",
        "  val_loss = test_epoch(encoder, decoder, device, train_loader, loss_fn, optim)\n",
        "\n",
        "  print('\\n Epoch {}/{} \\t train loss {} \\t val loss {}'.format(epoch+1, num_epochs, train_loss, val_loss))\n",
        "\n",
        "  diz_loss['train_loss'].append(train_loss)\n",
        "  diz_loss['val_loss'].append(val_loss)\n",
        "\n",
        "  plot_ae_outputs(encoder, decoder, n=10)"
      ],
      "metadata": {
        "id": "ZVFyxIHftnsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL EVAL-- USING TEST SET\n",
        "test_epoch(encoder, decoder, device, test_loader, loss_fn).item()"
      ],
      "metadata": {
        "id": "PolTXV860VmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RECONSTRUCTION LOSSES DECREASE OVER EPOCHS (VISUALIZATION)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.semilogy(diz_loss['train_loss'], label='Train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zB2TqzyU0hc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE NEW SAMPLES FROM RANDOM LATENT CODE\n",
        "    # we will sample randomly from a normal distribution with the mean and std of the encoded data\n",
        "    # then we'll pass the samples to the decoder to create reconstructed images\n",
        "\n",
        "def show_image(img):\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  # calc mean and std of latent (encoded/compressed) code, generated taking in test images as inputs\n",
        "  images, labels = iter(test_loader).next()\n",
        "  images = images.to(device)\n",
        "  latent = encoder(images)\n",
        "  latent = latent.cpu()\n",
        "\n",
        "  mean = latent.mean(dim=0)\n",
        "  print(mean)\n",
        "  std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "  print(std)\n",
        "\n",
        "  # sample latent vectors from the normal distribution\n",
        "  latent = torch.randn(128, d) * std + mean\n",
        "\n",
        "  # reconstruct images from the random latent vectors\n",
        "  latent = latent.to(device)\n",
        "  img_recon = decoder(latent)\n",
        "  img_recon = img_recon.cpu()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "  show_image(torchvision.utils.make_grid(img_recon[:100], 10, 5))\n",
        "  plt.show()\n",
        "\n",
        "# some of the digits make no sense\n",
        "# this is bc the autoencoder latent space is extremely irregular, which is why close points in the latent space can produce different and meaningless patterns\n",
        "# this is why the autoencoder isn't great for generative purposes\n"
      ],
      "metadata": {
        "id": "fqzp_5rl07JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZE LATENT SPACE WITH t-SNE\n",
        "\n",
        "# first, create encoded samples using the test set\n",
        "from tqdm import tqdm\n",
        "encoded_samples = []\n",
        "for sample in tqdm(test_dataset):\n",
        "  img = sample[0].unsqueeze(0).to(device)\n",
        "  label = sample[1]\n",
        "\n",
        "  # encode image\n",
        "  encoder.eval()\n",
        "  with torch.no_grad():\n",
        "    encoded_img = encoder(img)\n",
        "\n",
        "  # append to list\n",
        "  encoded_img = encoded_img.flatten().cpu().numpy()\n",
        "  encoded_sample = {f\"Enc. Variable {i}\":enc for i, enc in enumerate(encoded_img)}\n",
        "  encoded_sample['label'] = label\n",
        "  encoded_samples.append(encoded_sample)\n",
        "encoded_samples = pd.DataFrame(encoded_samples)\n",
        "encoded_samples\n",
        "\n",
        "\n",
        "# then, plot the latent space representation using plotly epxress library\n",
        "    # this is really really cool\n",
        "    # we can see a scatterplot of where the dots representing each digit lie, and their overlap\n",
        "import plotly.express as px\n",
        "px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)\n",
        "\n",
        "\n",
        "# then, use TSNE,\n",
        "# TSNE is used for dimensionality reduction in order to visualize the latent code in a 2d space\n",
        "    # this is why n_components = 2, for 2D visualization\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_results = tsne.fit_transform(encoded_samples.drop(['label'], axis=1))\n",
        "fig = px.scatter(tsne_results, x=0, y=1,\n",
        "                 color=encoded_samples.label.astype(str),\n",
        "                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n",
        "fig.show()\n",
        "# we can very clearly see that each digit distinguishes itself from another (with some exceptions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "590CK-Ft5DxE",
        "outputId": "48efe6c5-f03f-476a-da83-f799d6e05e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b575407e30f8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ToTensor.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qplKVv7X71ri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}